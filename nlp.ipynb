{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcAj3vEErrlWzTWf1O3oPO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RudraKhare/DeepLearning/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPUK5keXJhfd",
        "outputId": "6a39b0d1-ef33-41a7-cc0a-50198078aade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        " pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT2GhcVOL1CV",
        "outputId": "abe631b9-adc8-442f-bc44-28663ef4d467"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\" Hello Welcome to, krish naik's NLP tutorials.\n",
        "please do watch entire course! to become expert in NLP\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bL6x0UlGKDMo"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1ueXO1VKn9M",
        "outputId": "03b4d99a-b8a8-4976-e81a-629308ab476c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello Welcome to, krish naik's NLP tutorials.\n",
            "please do watch entire course! to become expert in NLP\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Tokenziation\n",
        "##Sentence --> Paragraph\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "VBcPY80IKqRY"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents=sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "ETUPr2CdLWPG"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Omw_uOuLbAm",
        "outputId": "e8c2f0d8-714f-4638-8d98-3532caaf8a6c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_sAjhvjMeN7",
        "outputId": "b9e75357-b607-4d34-9f2f-955dbde561e4"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello Welcome to, krish naik's NLP tutorials.\n",
            "please do watch entire course!\n",
            "to become expert in NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "2wPkKFI2MkWK"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIQbEAL7Mvvm",
        "outputId": "a2b19f30-90b9-447e-ac79-df27a3d969bb"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " 'to',\n",
              " ',',\n",
              " 'krish',\n",
              " 'naik',\n",
              " \"'s\",\n",
              " 'NLP',\n",
              " 'tutorials',\n",
              " '.',\n",
              " 'please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz47CtcmMy8T",
        "outputId": "ce398c08-3c7d-4c97-a042-e5f7808be017"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Welcome', 'to', ',', 'krish', 'naik', \"'s\", 'NLP', 'tutorials', '.']\n",
            "['please', 'do', 'watch', 'entire', 'course', '!']\n",
            "['to', 'become', 'expert', 'in', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\"]"
      ],
      "metadata": {
        "id": "tuq9mllTNM15"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Porter Stemmer"
      ],
      "metadata": {
        "id": "K5rlYYgBL8Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "mws9jKuhL3aV"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemming=PorterStemmer()"
      ],
      "metadata": {
        "id": "PSgm4ckGMFL9"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUHOJjKqMJdV",
        "outputId": "7fcaebfb-7f96-4754-ba3a-27e88cd3c5a0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat\n",
            "eat\n",
            "eaten\n",
            "write\n",
            "write\n",
            "program\n",
            "program\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RegexpStemmer Class"
      ],
      "metadata": {
        "id": "00XaTEWWUfT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "HzMkSDBDUezi"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer=RegexpStemmer(\"ing$|s$|e$|able$|\",min=4)"
      ],
      "metadata": {
        "id": "xCMQnvD2UaK9"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem(\"eating\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "SWo67G6aM0n8",
        "outputId": "961e05a3-af90-4c48-c2f8-19b392aafe22"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Snowball Stemmer"
      ],
      "metadata": {
        "id": "9rkiqqlUWmY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "Oibje8S_WDs6"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowballstemmer=SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "zefIgRQKWuWN"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(snowballstemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXG6wH1XW2w9",
        "outputId": "320810b4-24b8-422e-af3a-bca70d401473"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat\n",
            "eat\n",
            "eaten\n",
            "write\n",
            "write\n",
            "program\n",
            "program\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmetization"
      ],
      "metadata": {
        "id": "o1cqohhyXu9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "XbT19cuMXAVL"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg1ODKC5aZZi",
        "outputId": "a18c2642-bbca-4c75-a823-b7ebaa88249b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "m5SlhQGoYhTz"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"going\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "c_nupa39Yloj",
        "outputId": "a9ba4a10-5b9b-47b8-a1fc-980881896f8a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'going'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STOPWORDS"
      ],
      "metadata": {
        "id": "PEuTSoARcID_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"\"\" \"I HAVE THREE VISIONS FOR INDIA\"\n",
        "\n",
        "In 3000 years of our history, people from all over the world have come and invadedus, captured our lands, conquered our minds. From Alexander on-vvards. The Greeks,the Turks’, the-Moguls, the Portuguese, the British, the French, the Dutch, all of themcame and looted us, took over what was ours. Yet we have not done this to any othernation. We have not conquered anyone.\n",
        "\n",
        "We have not grabbed their land, their culture, their history and tried to enforce ourway of life on them. Why? Because we respect the freedom of others. That is why myfirst vision is that of FREEDOM. I believe that India got its first vision of this in 1857,when we started the war of independence. It is this freedom that we must protect andnurture and build on. If we are not free, no one will respect us.\n",
        "\n",
        "My second vision for India is DEVELOPMENT. For fifty years we have been adeveloping nation. It is time we see ourselves as a developed nation. We are amongthe top 5 nations of the world in terms of GDP. We have 10 percent growth rate in mostareas. Our poverty levels are falling. Our achievements are being globally recognizedtoday. Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "\n",
        "I HAVE A THIRD VISION\n",
        "\n",
        "India must stand up to the world. Because I believe that, unless India stands up tothe world, no one will respect us. Only STRENGTH respects strength. We must bestrong not only as a military power but also as an economics power. Both must gohand-in-hand. My good fortune was to have worked with three great minds. Dr. VikramSarabhai of the Dept of Space, Professor Sathish Dhawan, who succeded him and Dr.Brahm Prakash, father of nuclear material. I was lucky to have worked with all three ofthem closely and consider this the great opportunity of my life.\n",
        "\n",
        "I SEE FOUR MILESTONES IN MY CAREER:\n",
        "\n",
        "Twenty years I spent in ISRO. I was given the opportunity to be the Project Directorfor India’s first satellite launch vehicle, SLV-3. The one that launched Rohini. Theseyears played a very important role in my life of Scientist.\n",
        "\n",
        "After my ISRO years, I joined DRDO and got a chance to be the part of India’sguided missile program. It was my second bliss when Agni met its mission requirementsin 1994. The Dept. of Atomic Energy and DRDO had this tremendous partnership in therecent nuclear tests, on May 11 and 13. This was the third bliss. The joy of participating\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LLjZFmu1aCyD"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "c90a2-89dv7m"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "XTkSAO5xdzdM"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TP09u9ed4uL",
        "outputId": "ca1d133e-a3af-43aa-e294-bb21cd118782"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words(\"french\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6V4iJjod_xs",
        "outputId": "13193fb2-4484-40ad-e749-9c84c850f733"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['au',\n",
              " 'aux',\n",
              " 'avec',\n",
              " 'ce',\n",
              " 'ces',\n",
              " 'dans',\n",
              " 'de',\n",
              " 'des',\n",
              " 'du',\n",
              " 'elle',\n",
              " 'en',\n",
              " 'et',\n",
              " 'eux',\n",
              " 'il',\n",
              " 'ils',\n",
              " 'je',\n",
              " 'la',\n",
              " 'le',\n",
              " 'les',\n",
              " 'leur',\n",
              " 'lui',\n",
              " 'ma',\n",
              " 'mais',\n",
              " 'me',\n",
              " 'même',\n",
              " 'mes',\n",
              " 'moi',\n",
              " 'mon',\n",
              " 'ne',\n",
              " 'nos',\n",
              " 'notre',\n",
              " 'nous',\n",
              " 'on',\n",
              " 'ou',\n",
              " 'par',\n",
              " 'pas',\n",
              " 'pour',\n",
              " 'qu',\n",
              " 'que',\n",
              " 'qui',\n",
              " 'sa',\n",
              " 'se',\n",
              " 'ses',\n",
              " 'son',\n",
              " 'sur',\n",
              " 'ta',\n",
              " 'te',\n",
              " 'tes',\n",
              " 'toi',\n",
              " 'ton',\n",
              " 'tu',\n",
              " 'un',\n",
              " 'une',\n",
              " 'vos',\n",
              " 'votre',\n",
              " 'vous',\n",
              " 'c',\n",
              " 'd',\n",
              " 'j',\n",
              " 'l',\n",
              " 'à',\n",
              " 'm',\n",
              " 'n',\n",
              " 's',\n",
              " 't',\n",
              " 'y',\n",
              " 'été',\n",
              " 'étée',\n",
              " 'étées',\n",
              " 'étés',\n",
              " 'étant',\n",
              " 'étante',\n",
              " 'étants',\n",
              " 'étantes',\n",
              " 'suis',\n",
              " 'es',\n",
              " 'est',\n",
              " 'sommes',\n",
              " 'êtes',\n",
              " 'sont',\n",
              " 'serai',\n",
              " 'seras',\n",
              " 'sera',\n",
              " 'serons',\n",
              " 'serez',\n",
              " 'seront',\n",
              " 'serais',\n",
              " 'serait',\n",
              " 'serions',\n",
              " 'seriez',\n",
              " 'seraient',\n",
              " 'étais',\n",
              " 'était',\n",
              " 'étions',\n",
              " 'étiez',\n",
              " 'étaient',\n",
              " 'fus',\n",
              " 'fut',\n",
              " 'fûmes',\n",
              " 'fûtes',\n",
              " 'furent',\n",
              " 'sois',\n",
              " 'soit',\n",
              " 'soyons',\n",
              " 'soyez',\n",
              " 'soient',\n",
              " 'fusse',\n",
              " 'fusses',\n",
              " 'fût',\n",
              " 'fussions',\n",
              " 'fussiez',\n",
              " 'fussent',\n",
              " 'ayant',\n",
              " 'ayante',\n",
              " 'ayantes',\n",
              " 'ayants',\n",
              " 'eu',\n",
              " 'eue',\n",
              " 'eues',\n",
              " 'eus',\n",
              " 'ai',\n",
              " 'as',\n",
              " 'avons',\n",
              " 'avez',\n",
              " 'ont',\n",
              " 'aurai',\n",
              " 'auras',\n",
              " 'aura',\n",
              " 'aurons',\n",
              " 'aurez',\n",
              " 'auront',\n",
              " 'aurais',\n",
              " 'aurait',\n",
              " 'aurions',\n",
              " 'auriez',\n",
              " 'auraient',\n",
              " 'avais',\n",
              " 'avait',\n",
              " 'avions',\n",
              " 'aviez',\n",
              " 'avaient',\n",
              " 'eut',\n",
              " 'eûmes',\n",
              " 'eûtes',\n",
              " 'eurent',\n",
              " 'aie',\n",
              " 'aies',\n",
              " 'ait',\n",
              " 'ayons',\n",
              " 'ayez',\n",
              " 'aient',\n",
              " 'eusse',\n",
              " 'eusses',\n",
              " 'eût',\n",
              " 'eussions',\n",
              " 'eussiez',\n",
              " 'eussent']"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "asHZhzu6eJ_h"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "b2JhZtBPehv1"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "dD527xQWmd3W"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "jwxZQts_miKH"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "qf3mZizdekRk"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2bh_f_ien3J",
        "outputId": "93f3d7be-aa92-4dc6-cd05-e56364772588"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##APPLY STOPWORDS AND Filter and then apply stemming"
      ],
      "metadata": {
        "id": "h706FJYZgpWO"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words(\"english\"))]\n",
        "  sentences[i]=' '.join(words) #converting all list of the words into sentences"
      ],
      "metadata": {
        "id": "8VRywnm6gSKi"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN0X4DTQgkWp",
        "outputId": "edcdde2b-b91d-4c70-f873-7bf57502795e"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"`` I HAVE THREE VISIONS FOR INDIA '' In 3000 year history , people world come invadedus , captured land , conquered mind .\",\n",
              " 'From Alexander on-vvards .',\n",
              " 'The Greeks , Turks ’ , the-Moguls , Portuguese , British , French , Dutch , themcame looted u , took .',\n",
              " 'Yet done othernation .',\n",
              " 'We conquered anyone .',\n",
              " 'We grabbed land , culture , history tried enforce ourway life .',\n",
              " 'Why ?',\n",
              " 'Because respect freedom others .',\n",
              " 'That myfirst vision FREEDOM .',\n",
              " 'I believe India got first vision 1857 , started war independence .',\n",
              " 'It freedom must protect andnurture build .',\n",
              " 'If free , one respect u .',\n",
              " 'My second vision India DEVELOPMENT .',\n",
              " 'For fifty year adeveloping nation .',\n",
              " 'It time see developed nation .',\n",
              " 'We amongthe top 5 nation world term GDP .',\n",
              " 'We 10 percent growth rate mostareas .',\n",
              " 'Our poverty level falling .',\n",
              " 'Our achievement globally recognizedtoday .',\n",
              " 'Yet lack self-confidence see developed nation , self-reliant self-assured .',\n",
              " 'Isn ’ incorrect ?',\n",
              " 'I HAVE A THIRD VISION India must stand world .',\n",
              " 'Because I believe , unless India stand tothe world , one respect u .',\n",
              " 'Only STRENGTH respect strength .',\n",
              " 'We must bestrong military power also economics power .',\n",
              " 'Both must gohand-in-hand .',\n",
              " 'My good fortune worked three great mind .',\n",
              " 'Dr. VikramSarabhai Dept Space , Professor Sathish Dhawan , succeded Dr.Brahm Prakash , father nuclear material .',\n",
              " 'I lucky worked three ofthem closely consider great opportunity life .',\n",
              " 'I SEE FOUR MILESTONES IN MY CAREER : Twenty year I spent ISRO .',\n",
              " 'I given opportunity Project Directorfor India ’ first satellite launch vehicle , SLV-3 .',\n",
              " 'The one launched Rohini .',\n",
              " 'Theseyears played important role life Scientist .',\n",
              " 'After ISRO year , I joined DRDO got chance part India ’ sguided missile program .',\n",
              " 'It second bliss Agni met mission requirementsin 1994 .',\n",
              " 'The Dept .',\n",
              " 'Atomic Energy DRDO tremendous partnership therecent nuclear test , May 11 13 .',\n",
              " 'This third bliss .',\n",
              " 'The joy participating']"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "sentences=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "9LrvAb27tOZD"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn3JneFAtRQ6",
        "outputId": "a54e4d09-4fe4-42c7-aabf-9bde2c8093c3"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' \"I HAVE THREE VISIONS FOR INDIA\"\\n \\nIn 3000 years of our history, people from all over the world have come and invadedus, captured our lands, conquered our minds.',\n",
              " 'From Alexander on-vvards.',\n",
              " 'The Greeks,the Turks’, the-Moguls, the Portuguese, the British, the French, the Dutch, all of themcame and looted us, took over what was ours.',\n",
              " 'Yet we have not done this to any othernation.',\n",
              " 'We have not conquered anyone.',\n",
              " 'We have not grabbed their land, their culture, their history and tried to enforce ourway of life on them.',\n",
              " 'Why?',\n",
              " 'Because we respect the freedom of others.',\n",
              " 'That is why myfirst vision is that of FREEDOM.',\n",
              " 'I believe that India got its first vision of this in 1857,when we started the war of independence.',\n",
              " 'It is this freedom that we must protect andnurture and build on.',\n",
              " 'If we are not free, no one will respect us.',\n",
              " 'My second vision for India is DEVELOPMENT.',\n",
              " 'For fifty years we have been adeveloping nation.',\n",
              " 'It is time we see ourselves as a developed nation.',\n",
              " 'We are amongthe top 5 nations of the world in terms of GDP.',\n",
              " 'We have 10 percent growth rate in mostareas.',\n",
              " 'Our poverty levels are falling.',\n",
              " 'Our achievements are being globally recognizedtoday.',\n",
              " 'Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured.',\n",
              " 'Isn’t this incorrect?',\n",
              " 'I HAVE A THIRD VISION\\n \\nIndia must stand up to the world.',\n",
              " 'Because I believe that, unless India stands up tothe world, no one will respect us.',\n",
              " 'Only STRENGTH respects strength.',\n",
              " 'We must bestrong not only as a military power but also as an economics power.',\n",
              " 'Both must gohand-in-hand.',\n",
              " 'My good fortune was to have worked with three great minds.',\n",
              " 'Dr. VikramSarabhai of the Dept of Space, Professor Sathish Dhawan, who succeded him and Dr.Brahm Prakash, father of nuclear material.',\n",
              " 'I was lucky to have worked with all three ofthem closely and consider this the great opportunity of my life.',\n",
              " 'I SEE FOUR MILESTONES IN MY CAREER:\\n \\nTwenty years I spent in ISRO.',\n",
              " 'I was given the opportunity to be the Project Directorfor India’s first satellite launch vehicle, SLV-3.',\n",
              " 'The one that launched Rohini.',\n",
              " 'Theseyears played a very important role in my life of Scientist.',\n",
              " 'After my ISRO years, I joined DRDO and got a chance to be the part of India’sguided missile program.',\n",
              " 'It was my second bliss when Agni met its mission requirementsin 1994.',\n",
              " 'The Dept.',\n",
              " 'of Atomic Energy and DRDO had this tremendous partnership in therecent nuclear tests, on May 11 and 13.',\n",
              " 'This was the third bliss.',\n",
              " 'The joy of participating']"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6Hspo8atrp1",
        "outputId": "3aaf418c-2b8e-4027-b881-3868e96fe049"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[word for word in words if word not in set(stopwords.words(\"english\"))]\n",
        "  #sentences[i]=' '.join(words)\n",
        "  pos_tag=nltk.pos_tag(words)\n",
        "  print(pos_tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnWyCMcklm0G",
        "outputId": "31ebdcbe-bdd7-4a13-e8a8-9c8a9355eccd"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('``', '``'), ('I', 'PRP'), ('HAVE', 'VBP'), ('THREE', 'VBN'), ('VISIONS', 'NNP'), ('FOR', 'IN'), ('INDIA', 'NNP'), (\"''\", \"''\"), ('In', 'IN'), ('3000', 'CD'), ('years', 'NNS'), ('history', 'NN'), (',', ','), ('people', 'NNS'), ('world', 'NN'), ('come', 'VBP'), ('invadedus', 'NN'), (',', ','), ('captured', 'VBD'), ('lands', 'NNS'), (',', ','), ('conquered', 'VBD'), ('minds', 'NNS'), ('.', '.')]\n",
            "[('From', 'IN'), ('Alexander', 'NNP'), ('on-vvards', 'NNS'), ('.', '.')]\n",
            "[('The', 'DT'), ('Greeks', 'NNP'), (',', ','), ('Turks', 'NNP'), ('’', 'NNP'), (',', ','), ('the-Moguls', 'JJ'), (',', ','), ('Portuguese', 'NNP'), (',', ','), ('British', 'NNP'), (',', ','), ('French', 'NNP'), (',', ','), ('Dutch', 'NNP'), (',', ','), ('themcame', 'NN'), ('looted', 'VBD'), ('us', 'PRP'), (',', ','), ('took', 'VBD'), ('.', '.')]\n",
            "[('Yet', 'RB'), ('done', 'VBN'), ('othernation', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('conquered', 'VBD'), ('anyone', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('grabbed', 'VBD'), ('land', 'NN'), (',', ','), ('culture', 'NN'), (',', ','), ('history', 'NN'), ('tried', 'VBD'), ('enforce', 'NN'), ('ourway', 'RB'), ('life', 'NN'), ('.', '.')]\n",
            "[('Why', 'WRB'), ('?', '.')]\n",
            "[('Because', 'IN'), ('respect', 'NN'), ('freedom', 'NN'), ('others', 'NNS'), ('.', '.')]\n",
            "[('That', 'DT'), ('myfirst', 'VBZ'), ('vision', 'NN'), ('FREEDOM', 'NNP'), ('.', '.')]\n",
            "[('I', 'PRP'), ('believe', 'VBP'), ('India', 'NNP'), ('got', 'VBD'), ('first', 'JJ'), ('vision', 'NN'), ('1857', 'CD'), (',', ','), ('started', 'VBD'), ('war', 'NN'), ('independence', 'NN'), ('.', '.')]\n",
            "[('It', 'PRP'), ('freedom', 'NN'), ('must', 'MD'), ('protect', 'VB'), ('andnurture', 'NN'), ('build', 'NN'), ('.', '.')]\n",
            "[('If', 'IN'), ('free', 'JJ'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
            "[('My', 'PRP$'), ('second', 'JJ'), ('vision', 'NN'), ('India', 'NNP'), ('DEVELOPMENT', 'NNP'), ('.', '.')]\n",
            "[('For', 'IN'), ('fifty', 'JJ'), ('years', 'NNS'), ('adeveloping', 'VBG'), ('nation', 'NN'), ('.', '.')]\n",
            "[('It', 'PRP'), ('time', 'NN'), ('see', 'VB'), ('developed', 'JJ'), ('nation', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('amongthe', 'VBP'), ('top', 'JJ'), ('5', 'CD'), ('nations', 'NNS'), ('world', 'NN'), ('terms', 'NNS'), ('GDP', 'NNP'), ('.', '.')]\n",
            "[('We', 'PRP'), ('10', 'CD'), ('percent', 'JJ'), ('growth', 'NN'), ('rate', 'NN'), ('mostareas', 'NNS'), ('.', '.')]\n",
            "[('Our', 'PRP$'), ('poverty', 'NN'), ('levels', 'NNS'), ('falling', 'VBG'), ('.', '.')]\n",
            "[('Our', 'PRP$'), ('achievements', 'NNS'), ('globally', 'RB'), ('recognizedtoday', 'VBP'), ('.', '.')]\n",
            "[('Yet', 'RB'), ('lack', 'JJ'), ('self-confidence', 'NN'), ('see', 'NN'), ('developed', 'JJ'), ('nation', 'NN'), (',', ','), ('self-reliant', 'JJ'), ('self-assured', 'JJ'), ('.', '.')]\n",
            "[('Isn', 'NNP'), ('’', 'NNP'), ('incorrect', 'NN'), ('?', '.')]\n",
            "[('I', 'PRP'), ('HAVE', 'VBP'), ('A', 'DT'), ('THIRD', 'JJ'), ('VISION', 'NNP'), ('India', 'NNP'), ('must', 'MD'), ('stand', 'VB'), ('world', 'NN'), ('.', '.')]\n",
            "[('Because', 'IN'), ('I', 'PRP'), ('believe', 'VBP'), (',', ','), ('unless', 'IN'), ('India', 'NNP'), ('stands', 'VBZ'), ('tothe', 'JJ'), ('world', 'NN'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
            "[('Only', 'RB'), ('STRENGTH', 'NNP'), ('respects', 'VBZ'), ('strength', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('must', 'MD'), ('bestrong', 'VB'), ('military', 'JJ'), ('power', 'NN'), ('also', 'RB'), ('economics', 'VBZ'), ('power', 'NN'), ('.', '.')]\n",
            "[('Both', 'DT'), ('must', 'MD'), ('gohand-in-hand', 'NN'), ('.', '.')]\n",
            "[('My', 'PRP$'), ('good', 'JJ'), ('fortune', 'NN'), ('worked', 'VBD'), ('three', 'CD'), ('great', 'JJ'), ('minds', 'NNS'), ('.', '.')]\n",
            "[('Dr.', 'NNP'), ('VikramSarabhai', 'NNP'), ('Dept', 'NNP'), ('Space', 'NNP'), (',', ','), ('Professor', 'NNP'), ('Sathish', 'NNP'), ('Dhawan', 'NNP'), (',', ','), ('succeded', 'VBD'), ('Dr.Brahm', 'NNP'), ('Prakash', 'NNP'), (',', ','), ('father', 'RB'), ('nuclear', 'JJ'), ('material', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('lucky', 'VBP'), ('worked', 'VBD'), ('three', 'CD'), ('ofthem', 'NN'), ('closely', 'RB'), ('consider', 'VB'), ('great', 'JJ'), ('opportunity', 'NN'), ('life', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('SEE', 'VBP'), ('FOUR', 'JJ'), ('MILESTONES', 'NNP'), ('IN', 'NNP'), ('MY', 'NNP'), ('CAREER', 'NNP'), (':', ':'), ('Twenty', 'CD'), ('years', 'NNS'), ('I', 'PRP'), ('spent', 'VBD'), ('ISRO', 'NNP'), ('.', '.')]\n",
            "[('I', 'PRP'), ('given', 'VBN'), ('opportunity', 'NN'), ('Project', 'NNP'), ('Directorfor', 'NNP'), ('India', 'NNP'), ('’', 'NNP'), ('first', 'RB'), ('satellite', 'VBZ'), ('launch', 'JJ'), ('vehicle', 'NN'), (',', ','), ('SLV-3', 'NNP'), ('.', '.')]\n",
            "[('The', 'DT'), ('one', 'CD'), ('launched', 'VBD'), ('Rohini', 'NNP'), ('.', '.')]\n",
            "[('Theseyears', 'NNS'), ('played', 'VBD'), ('important', 'JJ'), ('role', 'NN'), ('life', 'NN'), ('Scientist', 'NNP'), ('.', '.')]\n",
            "[('After', 'IN'), ('ISRO', 'NNP'), ('years', 'NNS'), (',', ','), ('I', 'PRP'), ('joined', 'VBD'), ('DRDO', 'NNP'), ('got', 'VBD'), ('chance', 'NN'), ('part', 'NN'), ('India', 'NNP'), ('’', 'NNP'), ('sguided', 'VBD'), ('missile', 'NN'), ('program', 'NN'), ('.', '.')]\n",
            "[('It', 'PRP'), ('second', 'JJ'), ('bliss', 'JJ'), ('Agni', 'NNP'), ('met', 'VBD'), ('mission', 'NN'), ('requirementsin', 'NN'), ('1994', 'CD'), ('.', '.')]\n",
            "[('The', 'DT'), ('Dept', 'NNP'), ('.', '.')]\n",
            "[('Atomic', 'NNP'), ('Energy', 'NNP'), ('DRDO', 'NNP'), ('tremendous', 'JJ'), ('partnership', 'NN'), ('therecent', 'JJ'), ('nuclear', 'JJ'), ('tests', 'NNS'), (',', ','), ('May', 'NNP'), ('11', 'CD'), ('13', 'CD'), ('.', '.')]\n",
            "[('This', 'DT'), ('third', 'JJ'), ('bliss', 'NN'), ('.', '.')]\n",
            "[('The', 'DT'), ('joy', 'NN'), ('participating', 'VBG')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7TaUpGlxmAn9"
      },
      "execution_count": 141,
      "outputs": []
    }
  ]
}